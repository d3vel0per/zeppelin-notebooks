{"paragraphs":[{"text":"val sparkDummy = spark\r\n\r\nimport sparkDummy.implicits._","user":"anonymous","dateUpdated":"2019-02-03T17:45:16+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sparkDummy: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7c1c6748\nimport sparkDummy.implicits._\n"}]},"apps":[],"jobName":"paragraph_1549215706103_813706132","id":"20190203-195224_182147336","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:45:17+0000","dateFinished":"2019-02-03T17:45:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9766"},{"text":"%md\r\n# Comparing Structured Streaming with DStream\n\r\n<!-- We'll compare the Spark Structured Streaming and legacy DStream APIs to discuss their syntactic as well as semantic differences, and their relative strengths and weaknesses. -->\n\r\n\n\r\nThe old Spark streaming API used discretized streams or DStreams.  The model was that rather than a single, ever-growing dataset, a discretized stream provided an RDD at each trigger.  In a sense, it was a series of discretized batch operations.\n\r\n\n\r\n![](images/streaming-flow.png)\n\r\n\n\r\n<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->\n\r\n\n\r\nThe model had several drawbacks:\n\r\n1. DStream made it hard to deal with late or out-of-order data because the DStream was just discretized batches so it was difficult to update an old batch.\n\r\n1. The API for DStream was very different from the API for RDDs because their underlying data model was different.\n\r\n1. The streaming followed-by batch semantics made reliability hard.  If one step failed, the semantics of rerunning were unclear.\n\r\n\n\r\nNonetheless, it is still more built out than Structured Streaming and the API has feature support.  DStream will play a role in your streaming Spark applications for the foreseeable future.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Comparing Structured Streaming with DStream</h1>\n<!-- We'll compare the Spark Structured Streaming and legacy DStream APIs to discuss their syntactic as well as semantic differences, and their relative strengths and weaknesses. -->\n<p>The old Spark streaming API used discretized streams or DStreams. The model was that rather than a single, ever-growing dataset, a discretized stream provided an RDD at each trigger. In a sense, it was a series of discretized batch operations.</p>\n<p><img src=\"images/streaming-flow.png\" /></p>\n<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->\n<p>The model had several drawbacks:</p>\n<ol>\n  <li>\n  <p>DStream made it hard to deal with late or out-of-order data because the DStream was just discretized batches so it was difficult to update an old batch.</p></li>\n  <li>\n  <p>The API for DStream was very different from the API for RDDs because their underlying data model was different.</p></li>\n  <li>\n  <p>The streaming followed-by batch semantics made reliability hard. If one step failed, the semantics of rerunning were unclear.</p></li>\n</ol>\n<p>Nonetheless, it is still more built out than Structured Streaming and the API has feature support. DStream will play a role in your streaming Spark applications for the foreseeable future.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706141_-2027227183","id":"20190203-195224_497317311","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9767"},{"text":"%md\r\n# Custom Receivers in Spark DStream\n\r\n<!-- DStreams give the ability to build your own custom streaming source.  We demonstrate how to build a custom receiver. -->\n\r\n\n\r\nDStreams have support for custom receivers (i.e. sources).  For example, we are able to create a timed file source, which uses a separate thread to read data from a file and transmit the contents line by line, one each second.  The key is to extend the `Receiver` class and to use the `store` method to transmit.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Custom Receivers in Spark DStream</h1>\n<!-- DStreams give the ability to build your own custom streaming source.  We demonstrate how to build a custom receiver. -->\n<p>DStreams have support for custom receivers (i.e. sources). For example, we are able to create a timed file source, which uses a separate thread to read data from a file and transmit the contents line by line, one each second. The key is to extend the <code>Receiver</code> class and to use the <code>store</code> method to transmit.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706142_-651136936","id":"20190203-195224_1354283880","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9768"},{"text":"import scala.io.Source\r\nimport org.apache.spark.streaming.receiver.Receiver\r\nimport org.apache.spark.storage.StorageLevel\r\n\r\n// Should be a class ...\r\ndef timedFileSource(fileName: String) = {\r\n    new Receiver[String](StorageLevel.MEMORY_AND_DISK_2) {\r\n        def onStart() {\r\n            new Thread(\"Timed File Source\") {\r\n                override def run() { receive() }\r\n            }.start()\r\n        }\r\n\r\n        def onStop() { }\r\n\r\n        private def receive() {\r\n            for (line <- Source.fromFile(fileName).getLines) {\r\n                println(line)  // print for debugging\r\n                store(line)  // send the line as a source\r\n                Thread.sleep(1000L)  // Wait one second\r\n            }\r\n        }\r\n    }\r\n}","user":"anonymous","dateUpdated":"2019-02-03T17:45:28+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scala.io.Source\nimport org.apache.spark.streaming.receiver.Receiver\nimport org.apache.spark.storage.StorageLevel\ntimedFileSource: (fileName: String)org.apache.spark.streaming.receiver.Receiver[String]\n"}]},"apps":[],"jobName":"paragraph_1549215706158_178820184","id":"20190203-195224_1068988137","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:45:28+0000","dateFinished":"2019-02-03T17:45:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9769"},{"text":"%md\r\n# Iterative Wordcount Using Spark DStream\n\r\n<!-- One of the tricky aspects of DStreams is that while their API appears similar to that of RDDs, their semantics are not necessarily the obvious parallel.  We'll illustrate a streaming RDD example. -->\n\r\n\n\r\nTo illustrate the difference in APIs, we will write wordcount using DStream.  As we'll see, the syntax appears similar but the semantics are vastly different.\n\r\n![dstream-ops](images/streaming-dstream.png)\n\r\n\n\r\nBecause each discretized RDD is its own entity, we only see words in each time period and the wordcount will be per period, not the cumulative wordcount.\n\r\n![dstream-ops](images/streaming-dstream-ops.png)\n\r\n\n\r\n<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Iterative Wordcount Using Spark DStream</h1>\n<!-- One of the tricky aspects of DStreams is that while their API appears similar to that of RDDs, their semantics are not necessarily the obvious parallel.  We'll illustrate a streaming RDD example. -->\n<p>To illustrate the difference in APIs, we will write wordcount using DStream. As we&rsquo;ll see, the syntax appears similar but the semantics are vastly different.</p>\n<p><img src=\"images/streaming-dstream.png\" alt=\"dstream-ops\" /></p>\n<p>Because each discretized RDD is its own entity, we only see words in each time period and the wordcount will be per period, not the cumulative wordcount.</p>\n<p><img src=\"images/streaming-dstream-ops.png\" alt=\"dstream-ops\" /></p>\n<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706158_1463636130","id":"20190203-195224_1745477307","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9770"},{"text":"import org.apache.spark._\r\nimport org.apache.spark.streaming._\r\n\r\nval ssc = new StreamingContext(sc, Seconds(1))","user":"anonymous","dateUpdated":"2019-02-03T17:45:36+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport org.apache.spark.streaming._\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@5bb5f683\n"}]},"apps":[],"jobName":"paragraph_1549215706159_-1899113464","id":"20190203-195224_280612749","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:45:36+0000","dateFinished":"2019-02-03T17:45:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9771"},{"text":"(ssc.receiverStream(timedFileSource(\"/notebook/data/summer.txt\"))\r\n    .flatMap(_.split(\" \"))\r\n    .map(word => (word, 1))\r\n    .reduceByKey(_ + _)\r\n    .print())\r\n\r\nssc.start()             // Start the computation","user":"anonymous","dateUpdated":"2019-02-03T17:45:51+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1549215706167_11638878","id":"20190203-195224_452442615","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:45:51+0000","dateFinished":"2019-02-03T17:45:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9772"},{"text":"%md\r\n# Cumulative Wordcount using Spark DStream\n\r\n<!-- Keeping track of state between batch times for cumulative statistics is one of the more complex aspects of DStream.  We'll explain how and demonstrate it in a working sample.  -->\n\r\n\n\r\nThe above only gives the iterative (non-cumulative) wordcount, so how do we get the cumulative wordcount?  It turns out we need to keep track of state using separate `State` variables and manually update it in calls to `mapWithState`.  We will create a `StateSpec` object, which is a thin wrapper around our `updateFn` function.  This is called by `mapWithState` for key-value pairs.  It must perform two functions:\n\r\n\n\r\n1. Update the state variable that's passed in (side effects!)\n\r\n2. Return a resulting RDD\n\r\n\n\r\nThe API is definitely more involved and more cumbersome.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Cumulative Wordcount using Spark DStream</h1>\n<!-- Keeping track of state between batch times for cumulative statistics is one of the more complex aspects of DStream.  We'll explain how and demonstrate it in a working sample.  -->\n<p>The above only gives the iterative (non-cumulative) wordcount, so how do we get the cumulative wordcount? It turns out we need to keep track of state using separate <code>State</code> variables and manually update it in calls to <code>mapWithState</code>. We will create a <code>StateSpec</code> object, which is a thin wrapper around our <code>updateFn</code> function. This is called by <code>mapWithState</code> for key-value pairs. It must perform two functions:</p>\n<ol>\n  <li>\n  <p>Update the state variable that&rsquo;s passed in (side effects!)</p></li>\n  <li>\n  <p>Return a resulting RDD</p></li>\n</ol>\n<p>The API is definitely more involved and more cumbersome.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706167_1771814629","id":"20190203-195224_794693519","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9773"},{"text":"// You cannot create jobs in a started streaming context\r\n// Consider restarting the kernel\r\n\r\ndef updateFn(key: String, value: Option[Int], state: State[Int]) = {\r\n    // update state (stateful!)\r\n    state.update(value.getOrElse(0) + state.getOption.getOrElse(0))\r\n\r\n    // result to return\r\n    (key, state.get)\r\n}\r\n\r\nval spec = StateSpec.function(updateFn _)\r\n\r\n// checkpointing is mandatory\r\nssc.checkpoint(\"_checkpoints\")\r\n\r\n(ssc.receiverStream(timedFileSource(\"/notebook/data/summer.txt\"))\r\n    .flatMap(_.split(\" \"))\r\n    .map(word => (word, 1))\r\n    .mapWithState(spec)\r\n    .print())\r\n\r\nssc.start()             // Start the computation","user":"anonymous","dateUpdated":"2019-02-03T17:46:18+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\n  at org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:224)\n  at org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:66)\n  at org.apache.spark.streaming.dstream.InputDStream.<init>(InputDStream.scala:45)\n  at org.apache.spark.streaming.dstream.ReceiverInputDStream.<init>(ReceiverInputDStream.scala:42)\n  at org.apache.spark.streaming.dstream.PluggableInputDStream.<init>(PluggableInputDStream.scala:28)\n  at org.apache.spark.streaming.StreamingContext$$anonfun$receiverStream$1.apply(StreamingContext.scala:284)\n  at org.apache.spark.streaming.StreamingContext$$anonfun$receiverStream$1.apply(StreamingContext.scala:284)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.streaming.StreamingContext.withNamedScope(StreamingContext.scala:274)\n  at org.apache.spark.streaming.StreamingContext.receiverStream(StreamingContext.scala:283)\n  ... 61 elided\n"}]},"apps":[],"jobName":"paragraph_1549215706185_500629250","id":"20190203-195224_1438647069","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:46:18+0000","dateFinished":"2019-02-03T17:46:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9774"},{"text":"%md\r\n# Benefits of Spark Tungsten\n\r\n<!-- One of the benefits of Structured Streaming is that it takes advantage of recent low-level Spark optimizations.  We'll explain what these are in this segment. -->\n\r\n\n\r\nWe've seen the surface-level API differences between DStream and Structured Streaming. There are also many differences under the hood.  These have been added as newer versions of Spark for Datasets and Structured Streaming and are sometimes retrofitted to RDDs.\n\r\n\n\r\nSpark has traditionally been focused on optimizing for inter-computer network IO Efficiency.  This was the major bottleneck in MapReduce and other traditional distributed computing systems.  At this point, Spark is now intra-computer memory- and CPU-bound.\n\r\n\n\r\n1. **Customized Memory Management:** Spark understands its own memory allocation needs better than the generic JVM garbage collector.  Tungsten takes advantage of this by using `sun.misc.Unsafe`, which exposes C-Style off-heap memory access.  The result is fewer unnecessary garbage collection events and improved performance.\n\r\n1. **Binary Encoding:** While Spark traditionally needed to serialize data to JVM objects, it now uses the Tungsten binary encoding.  This has two major advantages.  First, it decreases the memory footprint.  While \"ABCD\" would take 4 bytes of UTF-8 to encode, it would be stored using 48 bytes as a JVM object.  Second, instead of serializing into objects, it's able to perform many actions directly on the the raw binary encoding, reducing the computational overhead for serialization / deserialization.\n\r\n1. **Code Generation:** Spark historically used generic JVM function evaluation.  Given the virtual function lookups, automated boxing of primitive types, and other JVM overhead, this dramatically slows down the computation.  By using type information, Tungsten is able to generate byte code and speed up performance.\n\r\n1. **Cache-aware Computation:** Tungsten lays out its memory in a way that takes advantage of CPU cache locality to reduce cache spilling and speed up computations.\n\r\n\n\r\nFor more information, check out [this blog post](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) or [this presentation](http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen).","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Benefits of Spark Tungsten</h1>\n<!-- One of the benefits of Structured Streaming is that it takes advantage of recent low-level Spark optimizations.  We'll explain what these are in this segment. -->\n<p>We&rsquo;ve seen the surface-level API differences between DStream and Structured Streaming. There are also many differences under the hood. These have been added as newer versions of Spark for Datasets and Structured Streaming and are sometimes retrofitted to RDDs.</p>\n<p>Spark has traditionally been focused on optimizing for inter-computer network IO Efficiency. This was the major bottleneck in MapReduce and other traditional distributed computing systems. At this point, Spark is now intra-computer memory- and CPU-bound.</p>\n<ol>\n  <li>\n  <p><strong>Customized Memory Management:</strong> Spark understands its own memory allocation needs better than the generic JVM garbage collector. Tungsten takes advantage of this by using <code>sun.misc.Unsafe</code>, which exposes C-Style off-heap memory access. The result is fewer unnecessary garbage collection events and improved performance.</p></li>\n  <li>\n  <p><strong>Binary Encoding:</strong> While Spark traditionally needed to serialize data to JVM objects, it now uses the Tungsten binary encoding. This has two major advantages. First, it decreases the memory footprint. While &ldquo;ABCD&rdquo; would take 4 bytes of UTF-8 to encode, it would be stored using 48 bytes as a JVM object. Second, instead of serializing into objects, it&rsquo;s able to perform many actions directly on the the raw binary encoding, reducing the computational overhead for serialization / deserialization.</p></li>\n  <li>\n  <p><strong>Code Generation:</strong> Spark historically used generic JVM function evaluation. Given the virtual function lookups, automated boxing of primitive types, and other JVM overhead, this dramatically slows down the computation. By using type information, Tungsten is able to generate byte code and speed up performance.</p></li>\n  <li>\n  <p><strong>Cache-aware Computation:</strong> Tungsten lays out its memory in a way that takes advantage of CPU cache locality to reduce cache spilling and speed up computations.</p></li>\n</ol>\n<p>For more information, check out <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">this blog post</a> or <a href=\"http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen\">this presentation</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706185_1179424702","id":"20190203-195224_1365216710","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9775"},{"text":"%md\r\n# Tungsten Performance Benefit Demonstration\n\r\n<!-- In this segment, we'll demonstrate how to use Spark's UI to view the memory usage of Spark and demonstrate how Spark Tungsten can reduce memory usage. -->\n\r\n\n\r\nThe following two operations count a million integers in memory.\n\r\n- The first uses RDDs and must serialize all the objects into Java Objects.\n\r\n- The second use sTungsten and uses a more compact binary encoding.\n\r\n\n\r\nTungsten saves a factor of nearly 4 on memory:\n\r\n\n\r\n![Tungsten Memory](images/Tungsten_Memory.png)\n\r\n\n\r\nTo reproduce the example, run the following code and go to your Spark UI Viewer.  This should be at [http://localhost:4040/storage/](http://localhost:4040/storage/) but may be at a different port if you have started multiple Spark kernels.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tungsten Performance Benefit Demonstration</h1>\n<!-- In this segment, we'll demonstrate how to use Spark's UI to view the memory usage of Spark and demonstrate how Spark Tungsten can reduce memory usage. -->\n<p>The following two operations count a million integers in memory.</p>\n<ul>\n  <li>\n  <p>The first uses RDDs and must serialize all the objects into Java Objects.</p></li>\n  <li>\n  <p>The second use sTungsten and uses a more compact binary encoding.</p></li>\n</ul>\n<p>Tungsten saves a factor of nearly 4 on memory:</p>\n<p><img src=\"images/Tungsten_Memory.png\" alt=\"Tungsten Memory\" /></p>\n<p>To reproduce the example, run the following code and go to your Spark UI Viewer. This should be at <a href=\"http://localhost:4040/storage/\">http://localhost:4040/storage/</a> but may be at a different port if you have started multiple Spark kernels.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706185_-775611161","id":"20190203-195224_1363709664","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9776"},{"text":"val million = sc.parallelize(0 until math.pow(10, 6).toInt)\r\n\r\n// Using RDDs\r\nmillion.cache.count\r\n\r\n// Using Tungsten\r\nmillion.toDS.cache.count","user":"anonymous","dateUpdated":"2019-02-03T17:47:10+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 32 cancelled as part of cancellation of all jobs\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n  ... 61 elided\n"}]},"apps":[],"jobName":"paragraph_1549215706186_939525147","id":"20190203-195224_369627315","dateCreated":"2019-02-03T17:41:46+0000","dateStarted":"2019-02-03T17:47:10+0000","dateFinished":"2019-02-09T10:10:23+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9777"},{"text":"%md\r\n# Benefits of Spark Catalyst\n\r\n<!-- In addition to the previous low-level optimizations of Tungsten, Structured Streaming also takes advantage of the Catalyst, which optimizes Spark jobs at the query level.  We'll explain what these are in this segment. -->\n\r\n\n\r\nIn addition to the previous low-level optimizations of Tungsten, Structured Streaming also takes advantage of the Catalyst, which optimizes Spark jobs at the query level.  Optimizations include:\n\r\n1. **Constant Folding:** Evaluating constant expressions at compile time rather than at run time.\n\r\n1. **Predicate Pushdown:** Running operations that reduce the dataload (e.g. selecting columns or filtering rows) earlier in the query.  This reduces the amount of data that needs to be processed.\n\r\n1. **Projection Pruning:** Only reading the columns (fields) used in the query from the database.\n\r\n1. **Pipelining Operations:** Combines multiple projection and filter operations into a single map operation.\n\r\n1. **Cost-based Optimization:** Spark actually builds multiple plans to compute a query, computes their cost, and chooses the cheapest one.\n\r\n1. **Code Generation:** The final operational plan is transformed into optimized Java bytecode.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Benefits of Spark Catalyst</h1>\n<!-- In addition to the previous low-level optimizations of Tungsten, Structured Streaming also takes advantage of the Catalyst, which optimizes Spark jobs at the query level.  We'll explain what these are in this segment. -->\n<p>In addition to the previous low-level optimizations of Tungsten, Structured Streaming also takes advantage of the Catalyst, which optimizes Spark jobs at the query level. Optimizations include:</p>\n<ol>\n  <li>\n  <p><strong>Constant Folding:</strong> Evaluating constant expressions at compile time rather than at run time.</p></li>\n  <li>\n  <p><strong>Predicate Pushdown:</strong> Running operations that reduce the dataload (e.g. selecting columns or filtering rows) earlier in the query. This reduces the amount of data that needs to be processed.</p></li>\n  <li>\n  <p><strong>Projection Pruning:</strong> Only reading the columns (fields) used in the query from the database.</p></li>\n  <li>\n  <p><strong>Pipelining Operations:</strong> Combines multiple projection and filter operations into a single map operation.</p></li>\n  <li>\n  <p><strong>Cost-based Optimization:</strong> Spark actually builds multiple plans to compute a query, computes their cost, and chooses the cheapest one.</p></li>\n  <li>\n  <p><strong>Code Generation:</strong> The final operational plan is transformed into optimized Java bytecode.</p></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706186_-1006558110","id":"20190203-195224_1694381076","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9778"},{"text":"%md\r\n# Viewing Query Plans in Spark Shell\n\r\n<!-- In this segment, we'll show how to debug query plans for the legacy RDDs and DStream APIs. -->\n\r\n\n\r\nFor an RDD, we can view the query plan calling the `.toDebugString` method.  Notice that multiple (even consecutive!) maps and filters are run as separate map and filter steps.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Viewing Query Plans in Spark Shell</h1>\n<!-- In this segment, we'll show how to debug query plans for the legacy RDDs and DStream APIs. -->\n<p>For an RDD, we can view the query plan calling the <code>.toDebugString</code> method. Notice that multiple (even consecutive!) maps and filters are run as separate map and filter steps.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706186_1288721698","id":"20190203-195224_1334347790","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9779"},{"text":"(million.map(_ + 1)\r\n    .map(_ + 1)\r\n    .filter(_ > 1)\r\n    .filter(_ > 2 * 4)\r\n    .toDebugString)","user":"anonymous","dateUpdated":"2019-02-03T17:44:56+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1549215706186_-1501625901","id":"20190203-195224_1512186001","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9780"},{"text":"%md\r\n# Visualizing Query Stages in Spark UI Viewer\n\r\n<!-- In this segment, we'll demonstrate how to use the Spark UI Viewer to view the individual stages of a Spark job. -->\n\r\n\n\r\nYou can also check out the \"Stages\" tab in the Spark UI viewer [http://localhost:4040/stages/](http://localhost:4040/stages/)  to check out the individual stages of a Spark job.\n\r\n\n\r\n<img src=\"images/spark-stages.png\" width=\"40%\"/>","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Visualizing Query Stages in Spark UI Viewer</h1>\n<!-- In this segment, we'll demonstrate how to use the Spark UI Viewer to view the individual stages of a Spark job. -->\n<p>You can also check out the &ldquo;Stages&rdquo; tab in the Spark UI viewer <a href=\"http://localhost:4040/stages/\">http://localhost:4040/stages/</a> to check out the individual stages of a Spark job.</p>\n<img src=\"images/spark-stages.png\" width=\"40%\"/>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706186_1702385686","id":"20190203-195224_217657070","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9781"},{"text":"%md\r\n# Viewing Spark Catalyst-Optimzied Physical Plans\n\r\n<!-- In this segment, we'll show how to debug query plans for the new Dataset and Structured Streaming APIs and show how Spark can optimize some fairly complex queries. -->\n\r\n\n\r\nFor a dataset, we can view the query plan with the `.explain` method.  You can see that it has performed several optimizations:\n\r\n1. It grouped multiple (even non-consecutive!) `select` and `filter` operations together.\n\r\n1. It pushed the `.filter` earlier in the query.\n\r\n1. It performs constant folding by multiplying out `2 * 4`.","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Viewing Spark Catalyst-Optimzied Physical Plans</h1>\n<!-- In this segment, we'll show how to debug query plans for the new Dataset and Structured Streaming APIs and show how Spark can optimize some fairly complex queries. -->\n<p>For a dataset, we can view the query plan with the <code>.explain</code> method. You can see that it has performed several optimizations:</p>\n<ol>\n  <li>\n  <p>It grouped multiple (even non-consecutive!) <code>select</code> and <code>filter</code> operations together.</p></li>\n  <li>\n  <p>It pushed the <code>.filter</code> earlier in the query.</p></li>\n  <li>\n  <p>It performs constant folding by multiplying out <code>2 * 4</code>.</p></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706186_1334120885","id":"20190203-195224_422766065","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9782"},{"text":"(million.toDS\r\n    .select('value, 'value + 1 as 'value2)\r\n    .select('value2 + 1 as 'value2)\r\n    .filter('value >= 1)\r\n    .select('value2 + 1 as 'value2)\r\n    .filter('value >= 2 * 4)\r\n    .explain)","user":"anonymous","dateUpdated":"2019-02-03T17:45:15+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1549215706187_-1800805035","id":"20190203-195224_309446177","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9783"},{"text":"%md\r\n<img src=\"images/logo-text.jpg\" width=\"20%\"/>","user":"anonymous","dateUpdated":"2019-02-03T17:41:46+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src=\"images/logo-text.jpg\" width=\"20%\"/>\n</div>"}]},"apps":[],"jobName":"paragraph_1549215706187_-94723746","id":"20190203-195224_1495026262","dateCreated":"2019-02-03T17:41:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9784"}],"name":"structured_streaming/Part_3_DStream_Comparison","id":"2E2PJBMMF","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}