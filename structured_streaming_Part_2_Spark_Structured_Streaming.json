{"paragraphs":[{"text":"%sh\n# Checking environment variables\necho \"JAVA_HOME: $JAVA_HOME\"\necho \"SPARK_HOME: $SPARK_HOME\"\necho \"HADOOP_CONF_DIR: $HADOOP_CONF_DIR\"","user":"anonymous","dateUpdated":"2019-09-22T20:08:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\nSPARK_HOME: \nHADOOP_CONF_DIR: \n"}]},"apps":[],"jobName":"paragraph_1569178128821_1039415413","id":"20190922-184848_833228530","dateCreated":"2019-09-22T18:48:48+0000","dateStarted":"2019-09-22T20:08:40+0000","dateFinished":"2019-09-22T20:08:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6558"},{"text":"%md\r\n# Spark Structured Streaming\r\n<!-- We give an overview of our deep dive into Spark Structured Streaming. -->\r\n\r\nRecall that we can think of Spark Structured Streaming as generating an infinite dataset against which we write queries.  We'll explore the API fully.\r\n- We'll demonstrate how to pull data from different streaming sources like websockets or files.\r\n- We'll delve into parsing and structuring data.\r\n- We'll demonstrate how to run queries against this data using the structured streaming API.","user":"anonymous","dateUpdated":"2019-09-22T18:51:25+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Structured Streaming</h1>\n<!-- We give an overview of our deep dive into Spark Structured Streaming. -->\n<p>Recall that we can think of Spark Structured Streaming as generating an infinite dataset against which we write queries. We&rsquo;ll explore the API fully.<br/>- We&rsquo;ll demonstrate how to pull data from different streaming sources like websockets or files.<br/>- We&rsquo;ll delve into parsing and structuring data.<br/>- We&rsquo;ll demonstrate how to run queries against this data using the structured streaming API.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104557_-1261084593","id":"20190203-195139_716791656","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T18:51:25+0000","dateFinished":"2019-09-22T18:51:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6559"},{"text":"val sparkDummy = spark\r\nimport sparkDummy.implicits._","user":"anonymous","dateUpdated":"2019-09-22T20:08:55+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sparkDummy: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@505bd1ef\nimport sparkDummy.implicits._\n"}]},"apps":[],"jobName":"paragraph_1549204104565_19691897","id":"20190203-195139_486650207","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:08:55+0000","dateFinished":"2019-09-22T20:10:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6560"},{"text":"%md\r\n# Netcat Socket Structured Streaming Example\n\r\n<!-- We will demonstrate how to write a small Scala script to broadcast a file to that port and how to invoke bash commands from Scala repls and notebooks for testing the broadcast server -->\n\r\n\n\r\nWe're going to stream one of Shakespeare's most famous poems on a fixed port and listen for it using Spark.  We set up a streaming server to broadcast the file one line at a time using [Broadcast.scala](/edit/Broadcast.scala).","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Netcat Socket Structured Streaming Example</h1>\n<!-- We will demonstrate how to write a small Scala script to broadcast a file to that port and how to invoke bash commands from Scala repls and notebooks for testing the broadcast server -->\n<p>We&rsquo;re going to stream one of Shakespeare&rsquo;s most famous poems on a fixed port and listen for it using Spark. We set up a streaming server to broadcast the file one line at a time using <a href=\"/edit/Broadcast.scala\">Broadcast.scala</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104565_299540241","id":"20190203-195139_1886800125","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6561"},{"text":"// create a stream and listen on a port\r\n\r\ndef createStream(port: Int, duration: Int) {\r\n    val lines = (spark.readStream\r\n        .format(\"socket\")\r\n        .option(\"host\", \"localhost\")\r\n        .option(\"port\", port)\r\n        .load())\r\n\r\n    val words = (lines\r\n        .as[String]\r\n        .flatMap(_.split(\"\\\\s+\")))\r\n\r\n    val wordCounts = (words\r\n        .groupByKey(_.toLowerCase)\r\n        .count()\r\n        .orderBy($\"count(1)\" desc))\r\n\r\n    val query = (wordCounts.writeStream\r\n        .outputMode(\"complete\")\r\n        .format(\"console\")\r\n        .start\r\n        .awaitTermination(duration))\r\n\r\n}","user":"anonymous","dateUpdated":"2019-09-22T19:42:56+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\ncreateStream: (port: Int, duration: Int)Unit\n"}]},"apps":[],"jobName":"paragraph_1549204104566_807019283","id":"20190203-195139_1930176273","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T19:42:56+0000","dateFinished":"2019-09-22T19:43:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6562"},{"text":"// run `nc -lk 12341` in bash and start typing!\r\n\r\ncreateStream(12341, 10000)","user":"anonymous","dateUpdated":"2019-02-03T15:30:46+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 2c476399-a730-4f8d-ab59-14f925e4b38d, runId = fc64eb85-0a8f-4223-b6bf-3a559b0b3ce4] terminated with exception: Connection refused (Connection refused)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:343)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:206)\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n  at java.net.PlainSocketImpl.socketConnect(Native Method)\n  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n  at java.net.Socket.connect(Socket.java:589)\n  at java.net.Socket.connect(Socket.java:538)\n  at java.net.Socket.<init>(Socket.java:434)\n  at java.net.Socket.<init>(Socket.java:211)\n  at org.apache.spark.sql.execution.streaming.TextSocketSource.initialize(socket.scala:73)\n  at org.apache.spark.sql.execution.streaming.TextSocketSource.<init>(socket.scala:70)\n  at org.apache.spark.sql.execution.streaming.TextSocketSourceProvider.createSource(socket.scala:215)\n  at org.apache.spark.sql.execution.datasources.DataSource.createSource(DataSource.scala:244)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2$$anonfun$applyOrElse$1.apply(StreamExecution.scala:158)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2$$anonfun$applyOrElse$1.apply(StreamExecution.scala:155)\n  at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:194)\n  at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.applyOrElse(StreamExecution.scala:155)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.applyOrElse(StreamExecution.scala:153)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.logicalPlan$lzycompute(StreamExecution.scala:153)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.logicalPlan(StreamExecution.scala:147)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:276)\n  ... 1 more\n"}]},"apps":[],"jobName":"paragraph_1549204104566_-2079475188","id":"20190203-195139_1879274853","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-02-03T15:30:46+0000","dateFinished":"2019-02-03T15:30:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6563"},{"text":"%md\r\n**Exercise:** type \"foo\", \"foo bar\",  and \"bar\", and watch the results ","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercise:</strong> type &ldquo;foo&rdquo;, &ldquo;foo bar&rdquo;, and &ldquo;bar&rdquo;, and watch the results </p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104566_-202799510","id":"20190203-195139_1926726418","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6564"},{"text":"%md\r\n# Socket Structured Streaming Example\n\r\n<!-- We demonstrate how to use sockets to listen on a fixed port and how to set up a simple netcat server to broadcast to that port. -->\n\r\n\n\r\nWe can also broadcast on Unix's `netcat` to broadcast a stream on a fixed port.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Socket Structured Streaming Example</h1>\n<!-- We demonstrate how to use sockets to listen on a fixed port and how to set up a simple netcat server to broadcast to that port. -->\n<p>We can also broadcast on Unix&rsquo;s <code>netcat</code> to broadcast a stream on a fixed port.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104567_1640734948","id":"20190203-195139_1199748545","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6565"},{"text":"import sys.process._\r\n\r\n\"more /notebook/data/summer.txt\" ! // run bash command using bang after a string","user":"anonymous","dateUpdated":"2019-09-22T19:43:12+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\n::::::::::::::\n/notebook/data/summer.txt\n::::::::::::::\nShall I compare thee to a summer’s day?\nThou art more lovely and more temperate.\nRough winds do shake the darling buds of May,\nAnd summer’s lease hath all too short a date.\nSometime too hot the eye of heaven shines,\nAnd often is his gold complexion dimmed;\nAnd every fair from fair sometime declines,\nBy chance, or nature’s changing course, untrimmed;\nBut thy eternal summer shall not fade,\nNor lose possession of that fair thou ow’st,\nNor shall death brag thou wand’rest in his shade,\nWhen in eternal lines to Time thou grow’st.\nSo long as men can breathe, or eyes can see,\nSo long lives this, and this gives life to thee.import sys.process._\nres1: Int = 0\n"}]},"apps":[],"jobName":"paragraph_1549204104567_-123284617","id":"20190203-195139_151216432","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T19:43:12+0000","dateFinished":"2019-09-22T19:43:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6566"},{"text":"val port = 12342\r\n\r\n// Broadcast file on port one line at a time\r\n\r\n(new Thread {\r\n    override def run {\r\n        s\"scala Broadcast.scala ${port} /notebook/data/summer.txt\" !\r\n    }\r\n}).start","user":"anonymous","dateUpdated":"2019-09-22T17:11:25+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\nport: Int = 12342\n"}]},"apps":[],"jobName":"paragraph_1549204104567_-1012873468","id":"20190203-195139_153886290","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T17:11:25+0000","dateFinished":"2019-09-22T17:11:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6567"},{"text":"createStream(port, 12000)","user":"anonymous","dateUpdated":"2019-09-22T17:11:30+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 69d6b803-84e7-4547-a813-05ab60ccb75b, runId = 4387c4ba-6ab5-4115-9560-abf319280fa1] terminated with exception: Connection refused (Connection refused)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:343)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:206)\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n  at java.net.PlainSocketImpl.socketConnect(Native Method)\n  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n  at java.net.Socket.connect(Socket.java:589)\n  at java.net.Socket.connect(Socket.java:538)\n  at java.net.Socket.<init>(Socket.java:434)\n  at java.net.Socket.<init>(Socket.java:211)\n  at org.apache.spark.sql.execution.streaming.TextSocketSource.initialize(socket.scala:73)\n  at org.apache.spark.sql.execution.streaming.TextSocketSource.<init>(socket.scala:70)\n  at org.apache.spark.sql.execution.streaming.TextSocketSourceProvider.createSource(socket.scala:215)\n  at org.apache.spark.sql.execution.datasources.DataSource.createSource(DataSource.scala:244)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2$$anonfun$applyOrElse$1.apply(StreamExecution.scala:158)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2$$anonfun$applyOrElse$1.apply(StreamExecution.scala:155)\n  at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:194)\n  at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.applyOrElse(StreamExecution.scala:155)\n  at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.applyOrElse(StreamExecution.scala:153)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.logicalPlan$lzycompute(StreamExecution.scala:153)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.logicalPlan(StreamExecution.scala:147)\n  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:276)\n  ... 1 more\n"}]},"apps":[],"jobName":"paragraph_1549204104568_235596181","id":"20190203-195139_803219531","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T17:11:30+0000","dateFinished":"2019-09-22T17:11:31+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6568"},{"text":"%md\r\n# Spark Structured Streaming Parsing Data\n\r\n<!-- Most manipulations we do will involve structuring data.  We demonstrate how to use case classes and Scala Reflection to easily structure our data and account for missing or incomplete fields. -->\n\r\n\n\r\nMuch as with datasets, we can use a `case class` to represent rows of data.  The case class's attributes correspond to the json field names or (as in this case) the CSV column names.\n\r\n\n\r\nHowever, unlike with datasets, we cannot ask the reader to infer the schema.  Instead, we will use `ScalaReflection` to generate a schema for our case class.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Structured Streaming Parsing Data</h1>\n<!-- Most manipulations we do will involve structuring data.  We demonstrate how to use case classes and Scala Reflection to easily structure our data and account for missing or incomplete fields. -->\n<p>Much as with datasets, we can use a <code>case class</code> to represent rows of data. The case class&rsquo;s attributes correspond to the json field names or (as in this case) the CSV column names.</p>\n<p>However, unlike with datasets, we cannot ask the reader to infer the schema. Instead, we will use <code>ScalaReflection</code> to generate a schema for our case class.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104568_-1348449853","id":"20190203-195139_2107550923","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6569"},{"text":"import sys.process._\r\n\r\n\"cat /notebook/data/people/1.csv\" ! // run bash command using bang after a string","user":"anonymous","dateUpdated":"2019-09-22T19:43:36+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\nname,city,country,age\nAmy,Paris,FR,30\nBob,New York,US,22\nCharlie,London,UK,35\nDenise,San Francisco,US,22\nimport sys.process._\nres2: Int = 0\n"}]},"apps":[],"jobName":"paragraph_1549204104568_1528722249","id":"20190203-195139_1871075497","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T19:43:36+0000","dateFinished":"2019-09-22T19:43:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6570"},{"text":"import org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.catalyst.ScalaReflection\r\nimport sparkDummy.implicits._\r\n\r\ncase class Person(\r\n    name: String,\r\n    city: String,\r\n    country: String,\r\n    age: Option[Int]\r\n)\r\n\r\n// create schema for parsing data\r\nval caseSchema = (ScalaReflection\r\n    .schemaFor[Person]\r\n    .dataType\r\n    .asInstanceOf[StructType])\r\n\r\nval peopleStream = (spark.readStream\r\n    .schema(caseSchema)\r\n    .option(\"header\", true)  // Headers are matched to Person properties\r\n    .option(\"maxFilesPerTrigger\", 1)  // each file is read in a separate batch\r\n    .csv(\"/notebook/data/people/\")  // load a CSV file\r\n    .as[Person])\r\n\r\n(peopleStream.writeStream\r\n    .outputMode(\"append\")  // write results to screen\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:12:03+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport sparkDummy.implicits._\ndefined class Person\ncaseSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(city,StringType,true), StructField(country,StringType,true), StructField(age,IntegerType,true))\npeopleStream: org.apache.spark.sql.Dataset[Person] = [name: string, city: string ... 2 more fields]\nres2: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@690735d5\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-------+-------------+-------+---+\n|   name|         city|country|age|\n+-------+-------------+-------+---+\n|    Amy|        Paris|     FR| 30|\n|    Bob|     New York|     US| 22|\n|Charlie|       London|     UK| 35|\n| Denise|San Francisco|     US| 22|\n+-------+-------------+-------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1549204104569_-979231958","id":"20190203-195139_103880069","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:12:15+0000","dateFinished":"2019-09-22T20:12:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6571"},{"text":"%md\r\n**Exercises:**\r\n\r\n- What would happen if age were not optional?\r\n- What if the age were \"five\" instead of \"5\"?\r\n- What if one of the records was missing an \"age\" record?\r\n- There's also a `.json` method that we could use in lieu of the `.csv` method. Can you guess the json schema that this code would read?","user":"anonymous","dateUpdated":"2019-09-22T17:18:03+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercises:</strong></p>\n<ul>\n  <li>What would happen if age were not optional?</li>\n  <li>What if the age were &ldquo;five&rdquo; instead of &ldquo;5&rdquo;?</li>\n  <li>What if one of the records was missing an &ldquo;age&rdquo; record?</li>\n  <li>There&rsquo;s also a <code>.json</code> method that we could use in lieu of the <code>.csv</code> method. Can you guess the json schema that this code would read?</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104569_804834964","id":"20190203-195139_675141496","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T17:18:03+0000","dateFinished":"2019-09-22T17:18:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6572"},{"text":"%md\r\n# Constructing Columns in Structured Streaming\r\n<!-- Structured Streaming makes heavy use of Column objects for manipulating data.  In this section, we explain various ways in which the Column objects can be constructed from columns in our structured stream or by combining other columns. -->\r\n\r\nDatasets use a dataframe syntax to refer to columns (which are themselves `Column` objects).  There are a number of ways to do this:\r\n- `peopleStream(\"country\")`\r\n- `peopleStream.col(\"country\")`\r\n- `$\"country\"`\r\n- `'country`\r\n\r\nThe first two are more explicit as they tell Spark which data stream to use.  This is useful in joins when we want to specify the table more explicitly.  The second two are more implicit as they do not specify the data stream.  These are more useful for single datastream operations.  The symbols need to be imported from `spark.implicits`.\r\n\r\nThere are actually multiple ways to construct columns:\r\n- The above allows us to reference `Column`s already in a dataframe.\r\n- We can also construct a `Column` from other `Column`s using binary operators like `===` (equality), `>`, `<=`, `.plus`, `-`, `.startsWith`, or `&&`, depending on the underlying value of the column.\r\n- Finally, we can rename the columns (keeping the values) with the operator `as`.","user":"anonymous","dateUpdated":"2019-09-22T18:14:02+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Constructing Columns in Structured Streaming</h1>\n<!-- Structured Streaming makes heavy use of Column objects for manipulating data.  In this section, we explain various ways in which the Column objects can be constructed from columns in our structured stream or by combining other columns. -->\n<p>Datasets use a dataframe syntax to refer to columns (which are themselves <code>Column</code> objects). There are a number of ways to do this:<br/>- <code>peopleStream(&quot;country&quot;)</code><br/>- <code>peopleStream.col(&quot;country&quot;)</code><br/>- <code>$&quot;country&quot;</code><br/>- <code>&#39;country</code></p>\n<p>The first two are more explicit as they tell Spark which data stream to use. This is useful in joins when we want to specify the table more explicitly. The second two are more implicit as they do not specify the data stream. These are more useful for single datastream operations. The symbols need to be imported from <code>spark.implicits</code>.</p>\n<p>There are actually multiple ways to construct columns:<br/>- The above allows us to reference <code>Column</code>s already in a dataframe.<br/>- We can also construct a <code>Column</code> from other <code>Column</code>s using binary operators like <code>===</code> (equality), <code>&gt;</code>, <code>&lt;=</code>, <code>.plus</code>, <code>-</code>, <code>.startsWith</code>, or <code>&amp;&amp;</code>, depending on the underlying value of the column.<br/>- Finally, we can rename the columns (keeping the values) with the operator <code>as</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104569_1026618776","id":"20190203-195139_550362688","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T18:12:37+0000","dateFinished":"2019-09-22T18:12:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6573"},{"text":"%md\r\n# Selecting and Filtering Columns Using Structured Streaming\n\r\n<!-- We'll demonstrate how to select and filter columns using Structured Streaming. -->\n\r\n\n\r\nWe'll demonstrate these using the `select` method, which takes any non-zero number of `Column` arguments and returns a dataframe with those arguments.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Selecting and Filtering Columns Using Structured Streaming</h1>\n<!-- We'll demonstrate how to select and filter columns using Structured Streaming. -->\n<p>We&rsquo;ll demonstrate these using the <code>select</code> method, which takes any non-zero number of <code>Column</code> arguments and returns a dataframe with those arguments.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104569_-1192241651","id":"20190203-195139_1727217115","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6574"},{"text":"import sparkDummy.implicits._\r\n\r\n(peopleStream.select(\r\n    $\"country\" === \"UK\" as \"in_UK\",\r\n    $\"age\" <= 30 as \"under_30\",\r\n    'country startsWith \"U\" as \"U_Country\")\r\n        .writeStream\r\n        .outputMode(\"append\")  // write results to screen\r\n        .format(\"console\")\r\n        .start)","user":"anonymous","dateUpdated":"2019-09-22T20:15:46+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sparkDummy.implicits._\nres4: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4e5cb794\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+--------+---------+\n|in_UK|under_30|U_Country|\n+-----+--------+---------+\n|false|    true|    false|\n|false|    true|     true|\n| true|   false|     true|\n|false|    true|     true|\n+-----+--------+---------+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----+--------+---------+\n|in_UK|under_30|U_Country|\n+-----+--------+---------+\n| true|   false|     true|\n|false|    true|    false|\n| true|    null|     true|\n+-----+--------+---------+\n\n"}]},"apps":[],"jobName":"paragraph_1549204104570_-1798943277","id":"20190203-195139_610789332","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:15:46+0000","dateFinished":"2019-09-22T20:15:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6575"},{"text":"import sparkDummy.implicits._\r\n\r\n(peopleStream.filter($\"age\" === 22)\r\n    .writeStream\r\n    .outputMode(\"append\")  // write results to screen\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:16:00+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sparkDummy.implicits._\nres5: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@173e5ebc\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+------+-------------+-------+---+\n|  name|         city|country|age|\n+------+-------------+-------+---+\n|   Bob|     New York|     US| 22|\n|Denise|San Francisco|     US| 22|\n+------+-------------+-------+---+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-------+----+-------+---+\n|   name|city|country|age|\n+-------+----+-------+---+\n|Francis|null|     FR| 22|\n+-------+----+-------+---+\n\n"}]},"apps":[],"jobName":"paragraph_1549204104570_-1084420099","id":"20190203-195139_32401199","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:16:00+0000","dateFinished":"2019-09-22T20:16:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6576"},{"text":"%md\r\n**Exercises:**\r\n\r\n1. Select the column \"age + 1\".\r\n1. Select a column that returns true if the user is a Londoner who is under 30 as \"Young_Londoner\".\r\n1. Filter for when the age is no less than 22.\r\n1. Filter for the city being \"London\".\r\n1. Filter for Americans under the age of 30.","user":"anonymous","dateUpdated":"2019-09-22T19:50:33+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercises:</strong></p>\n<ol>\n  <li>Select the column &ldquo;age + 1&rdquo;.</li>\n  <li>Select a column that returns true if the user is a Londoner who is under 30 as &ldquo;Young_Londoner&rdquo;.</li>\n  <li>Filter for when the age is no less than 22.</li>\n  <li>Filter for the city being &ldquo;London&rdquo;.</li>\n  <li>Filter for Americans under the age of 30.</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104570_769263362","id":"20190203-195139_1721742785","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T19:50:33+0000","dateFinished":"2019-09-22T19:50:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6577"},{"text":"%md\r\n# GroupBy and Aggregation in Structured Streaming\r\n<!-- We'll demonstrate how to perform groupBy and data aggregation in Structured Streaming.  We will also demonstrate how to use groupBy on multiple columns. -->\r\n\r\nWe can use groupBy and aggregation as we would in SQL.\r\n- `groupBy` takes one or more `Column`s along which to groupBy.\r\n- The resulting object supports various built-in aggregation functions (`avg`, `mean`, `min`, `max`, `sum`) which take one or more string column names along which to aggregate.","user":"anonymous","dateUpdated":"2019-09-22T19:51:05+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>GroupBy and Aggregation in Structured Streaming</h1>\n<!-- We'll demonstrate how to perform groupBy and data aggregation in Structured Streaming.  We will also demonstrate how to use groupBy on multiple columns. -->\n<p>We can use groupBy and aggregation as we would in SQL.<br/>- <code>groupBy</code> takes one or more <code>Column</code>s along which to groupBy.<br/>- The resulting object supports various built-in aggregation functions (<code>avg</code>, <code>mean</code>, <code>min</code>, <code>max</code>, <code>sum</code>) which take one or more string column names along which to aggregate.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104571_-279382189","id":"20190203-195139_1637557418","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T19:51:05+0000","dateFinished":"2019-09-22T19:51:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6578"},{"text":"import sparkDummy.implicits._\r\n\r\n(peopleStream.groupBy($\"country\")\r\n    .mean(\"age\")\r\n    .writeStream\r\n    .outputMode(\"complete\")\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:29:02+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sparkDummy.implicits._\nres20: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@23736e71\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1549204104571_1980113501","id":"20190203-195139_919689423","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:29:03+0000","dateFinished":"2019-09-22T20:29:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6579"},{"text":"%md\r\nFor more complex aggregations, we can use `.agg`, which takes columns with aggregations.  Notice that we can reuse the keyword `as`, as well as other binary column operators from before.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>For more complex aggregations, we can use <code>.agg</code>, which takes columns with aggregations. Notice that we can reuse the keyword <code>as</code>, as well as other binary column operators from before.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104571_-342540914","id":"20190203-195139_110056528","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6580"},{"text":"import sparkDummy.implicits._\r\n\r\n(peopleStream.groupBy($\"city\")\r\n    .agg(first(\"country\") as \"country\", count(\"age\"))\r\n    .writeStream\r\n    .outputMode(\"complete\")\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:29:35+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sparkDummy.implicits._\nres21: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7608da97\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1549204104572_-1049006810","id":"20190203-195139_551801120","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:29:37+0000","dateFinished":"2019-09-22T20:29:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6581"},{"text":"%md\r\n**Exercise:** Add the average age of each city to the above query.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercise:</strong> Add the average age of each city to the above query.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104572_-728083803","id":"20190203-195139_593816152","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6582"},{"text":"%md\r\n# Joining Structured Stream with Datasets\r\n<!-- One of the best features of Structured Stream is the ability to natively join batch data with a Structured Stream. -->\r\n\r\nWe can join datastreams with datasets.  Remember: both of these are distributed datasets and one is being streamed -- that's a lot of semantics for a simple `.join` operator!\r\nBelow, we take a fixed user table and join it in with a stream of transactions in a fictitious poultry ecommerce website.","user":"anonymous","dateUpdated":"2019-09-22T20:17:19+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Joining Structured Stream with Datasets</h1>\n<!-- One of the best features of Structured Stream is the ability to natively join batch data with a Structured Stream. -->\n<p>We can join datastreams with datasets. Remember: both of these are distributed datasets and one is being streamed &ndash; that&rsquo;s a lot of semantics for a simple <code>.join</code> operator!<br/>Below, we take a fixed user table and join it in with a stream of transactions in a fictitious poultry ecommerce website.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104572_1183513062","id":"20190203-195139_533715419","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:17:19+0000","dateFinished":"2019-09-22T20:17:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6583"},{"text":"import org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.catalyst.ScalaReflection\r\nimport sparkDummy.implicits._\r\n\r\ncase class User(id: Int, name: String, email: String, country: String)\r\ncase class Transaction(userid: Int, product: String, cost: Double)\r\n\r\n// A user dataset\r\n// Notice that we do not have to provide a schema\r\n// We can simply infer it\r\nval users = (spark.read\r\n    .option(\"inferSchema\", \"true\")\r\n    .option(\"header\", true)\r\n    .csv(\"/notebook/data/users.csv\")\r\n    .as[User]\r\n)\r\n\r\nval transactionSchema = (ScalaReflection\r\n    .schemaFor[Transaction]\r\n    .dataType\r\n    .asInstanceOf[StructType]\r\n)\r\n\r\n// A stream of transactions\r\nval transactionStream = (spark.readStream\r\n    .schema(transactionSchema)\r\n    .option(\"header\", true)\r\n    .option(\"maxFilesPerTrigger\", 1)\r\n    .csv(\"/notebook/data/transactions/*.csv\")\r\n    .as[Transaction]\r\n)\r\n\r\n// Join transaction stream with user dataset\r\nval spendingByCountry = (transactionStream\r\n    .join(users, users(\"id\") === transactionStream(\"userid\"))\r\n    .groupBy($\"country\")\r\n    .agg(sum($\"cost\")) as \"spending\")\r\n\r\n// Print result\r\n(spendingByCountry.writeStream\r\n    .outputMode(\"complete\")\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:19:08+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport sparkDummy.implicits._\ndefined class User\ndefined class Transaction\nusers: org.apache.spark.sql.Dataset[User] = [id: int, name: string ... 2 more fields]\ntransactionSchema: org.apache.spark.sql.types.StructType = StructType(StructField(userid,IntegerType,false), StructField(product,StringType,true), StructField(cost,DoubleType,false))\ntransactionStream: org.apache.spark.sql.Dataset[Transaction] = [userid: int, product: string ... 1 more field]\nspendingByCountry: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, sum(cost): double]\nres11: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.Stre...-------------------------------------------\nBatch: 0\n-------------------------------------------\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=40","http://172.17.0.2:4040/jobs/job?id=41"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1549204104573_2091482384","id":"20190203-195139_284444508","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:19:08+0000","dateFinished":"2019-09-22T20:19:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6584"},{"text":"%md\r\n**Exercises:**\r\n- Show sales by product rather than country.\r\n- Show sales by both product and country.","user":"anonymous","dateUpdated":"2019-09-22T20:19:41+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercises:</strong><br/>- Show sales by product rather than country.<br/>- Show sales by both product and country.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104573_1917481914","id":"20190203-195139_742562391","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:19:41+0000","dateFinished":"2019-09-22T20:19:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6585"},{"text":"%md\r\n# SQL Queries in Spark Structured Streaming\n\r\n<!-- Spark also has an escape hatch into SQL queries that allows users to write familiar SQL queries against Structured Streams. -->\n\r\n\n\r\nFinally we can use the method `createOrReplaceTempView` to publish streams (and static datasets) as SQL tables.  We can then query the resulting table using SQL and stream the output as we would with any other datastream.","user":"anonymous","dateUpdated":"2019-02-03T14:28:24+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"fontSize":9,"enabled":true,"results":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>SQL Queries in Spark Structured Streaming</h1>\n<!-- Spark also has an escape hatch into SQL queries that allows users to write familiar SQL queries against Structured Streams. -->\n<p>Finally we can use the method <code>createOrReplaceTempView</code> to publish streams (and static datasets) as SQL tables. We can then query the resulting table using SQL and stream the output as we would with any other datastream.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104573_-1301148523","id":"20190203-195139_1398982951","dateCreated":"2019-02-03T14:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6586"},{"text":"import sparkDummy.implicits._\r\n\r\n// Publish SQL table\r\npeopleStream.createOrReplaceTempView(\"peopleTable\")\r\n\r\n// SQL query\r\nval query = spark.sql(\"SELECT country, avg(age) FROM peopleTable GROUP BY country\")\r\n\r\n// Output\r\n(query.writeStream\r\n    .outputMode(\"complete\")\r\n    .format(\"console\")\r\n    .start)","user":"anonymous","dateUpdated":"2019-09-22T20:20:11+0000","config":{"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sparkDummy.implicits._\nquery: org.apache.spark.sql.DataFrame = [country: string, avg(age): double]\nres12: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3739da03\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1549204104574_-1041927846","id":"20190203-195139_334691673","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:20:11+0000","dateFinished":"2019-09-22T20:20:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6587"},{"text":"%md\r\n**Exercise**:\r\n- Use the SQL syntax to filter for Londoners under 40 years.\r\n- Use the SQL syntax to join the user table and transaction stream to get transactions by country and product.","user":"anonymous","dateUpdated":"2019-09-22T20:20:31+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Exercise</strong>:<br/>- Use the SQL syntax to filter for Londoners under 40 years.<br/>- Use the SQL syntax to join the user table and transaction stream to get transactions by country and product.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104574_-7817081","id":"20190203-195139_409958102","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:20:31+0000","dateFinished":"2019-09-22T20:20:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6588"},{"text":"%md\r\n<img src=\"file://D:/development/zeppelin/notebook/images/logo-text.jpg\" width=\"20%\"/>","user":"anonymous","dateUpdated":"2019-09-22T20:22:58+0000","config":{"editorMode":"ace/mode/markdown","editorHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"enabled":true,"results":{},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src=\"file://D:/development/zeppelin/notebook/images/logo-text.jpg\" width=\"20%\"/>\n</div>"}]},"apps":[],"jobName":"paragraph_1549204104574_-1329871299","id":"20190203-195139_1019514778","dateCreated":"2019-02-03T14:28:24+0000","dateStarted":"2019-09-22T20:22:58+0000","dateFinished":"2019-09-22T20:22:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6589"}],"name":"structured_streaming/Part_2_Spark_Structured_Streaming","id":"2E4YZMWS6","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}